<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jinxiu Liu</title>
  
  <meta name="author" content="Jinxiu Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><i>Brandon Jinxiu Liu (ÂàòÈî¶Áª£)</i></name>
              </p>
              <p style="font-size: 16px;"> <b><i>‚ÄúStay hungry, Stay foolish‚Äù -- Steven Jobs</i> </b>
</p>
              
              <p style="font-size: 16px;">Hi there! I am an senior undergraduate student from South China University of Technology. Now I am working closely with <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=en"> Prof.Ming-Hsuan Yang </a> and
		      <a href="https://scholar.google.com/citations?user=kZsIU74AAAAJ&hl=en"> Dr. Yinxiao Li </a> from Google DeepMind. 
		      I spent a wonderful summer at <a href="https://svl.stanford.edu/"> Stanford AI Lab</a>.
                 I have also worked at Westlake University & OPPO Research Institude, focusing on Multi-modal LLM enhanced <b>Diffusion Model based Image/Video Generation</b>, 
		      advised by <a href="https://scholar.google.com/citations?hl=zh-CN&user=Nut-uvoAAAAJ&view_op=list_works&sortby=pubdate"> Prof. Guo-Jun Qi (IEEE Fellow) </a>.
                                <br>
<!-- 		      I am also a part-time consultant for the industry research team at <a href="https://www.miracleplus.com/en/"> Miracle Plus </a>  (formerly known as Y Combinator China). I aim to gain insights from industry demands to inform my research. -->
		        <br> 
		      
		    My long-term research goal is to build an AI system capable of <b>controllably</b> creating <b>immersive</b>, <b>interactive</b> and <b>physically</b> grounded 2D/3D/4D virtual worlds with the power of foundation model (LLM/MLLM, Image/Video/3D Diffusion), especially drawing inspiration from <b>human nature</b> and <b>the real needs of designers and artists</b>.
		      I envision my upcoming PhD journey as an entrepreneurial venture‚Äîsimilar to founding a startup‚Äîdriven by my "North Star" and focused on producing impactful, practical research.
  <br> 
      
 <br> 
            <font color="#FF0000">Sincerely looking for PhD positions for fall 2025 admission!    </font>

   <br>      </p>
              <p style="font-size: 16px;">
                Email: jinxiuliu0628@gmail.com &nbsp/&nbsp branodnjinxiuliu@cs.stanford.edu
                <br>
                Tel/Wechat: +86-13951891694
              </p>
              <p style="text-align:center;">
                <a href="mailto:branodnjinxiuliu@cs.stanford.edu">Email</a> &nbsp/&nbsp
                <a href="assets/Jinxiu_Liu_resume.pdf">CV</a> 
                &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=HsZpy3gAAAAJ&hl=zh-CN">Google Scholar</a> 
                <!-- <a href="https://github.com/Brandon-Liu-Jx">Github</a> &nbsp/&nbsp -->
                <!-- <a href="https://twitter.com/weixi_feng">Twitter</a> &nbsp/&nbsp -->

              </p>
            </td>

            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="javascript:void(0)" onclick="var img = this.children[0]; if (img.tagName === 'IMG' && img.src.includes('life.jpg')) { img.src = 'images/brandon_img.gif'; } else if (img.tagName === 'IMG' && img.src.includes('brandon_img.gif')) { img.src = 'images/life.jpg'; }">
                  <img style="width:100%;max-width:100%" alt="profile photo" src="images/life.jpg" class="hoverZoomLink">
                  
              </a>
              <br>
              <font color="#FF0000">Sidelights!!!</font> Click the portrait üëÜ and enjoy the animation magic from my project <b><font color="#FF0000">P</font>rompt <font color="#FF0000">i</font>mage to <font color="#FF0000">Life</font></b>.
          </td>
          
            
          </tr>
        </tbody></table>

	      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading style="font-size: 26px;"><i>Research Statement</i></heading>
        <br>
        <br>
        Recently I am considering how to develop an efficient and interactive world simulator with versatile applications in gaming, robotics, architecture, and physical simulations. This research is structured around three sub-goals, culminating in a dual-engine-driven simulator designed to optimize computational efficiency and interaction capabilities, even in resource-constrained and large-scale scenarios.
        <br>
        <b>Sub-Goal 1:Efficient Large Foundation Model</b>
        A key challenge in generative models lies in their high computational demands. My research aims to address this by creating efficient foundation models that deliver fast inference, use fewer parameters, and maintain high-quality outputs. Techniques such as distillation, pruning, and quantization will be employed to optimize autoregressive video generation models. These advancements are particularly crucial for real-time applications in AR and XR, where low latency and scalability across resolutions are essential for delivering immersive experiences across diverse devices.
        <br>
        <b>Sub-Goal 2: World Simulator based Generative Engine</b>
        This sub-goal focuses on building a world simulator capable of multi-modal, controllable visual generation while simulating complex world dynamics. The simulator will enable adaptive, interactive, and procedurally generated environments tailored to applications in gaming, robotics, and scientific research (AI4SCI). By integrating symbolic control and spatial reasoning, the system will dynamically adapt to user inputs and real-world constraints, enhancing interactivity, realism, and user engagement.
        <br>
        <b>Sub-Goal 3: Inverse Rendering and Physics Engine Based Generative Engine</b>
        Bridging the gap between generative diversity and structural precision is critical for creating high-quality, editable 3D and 4D assets. My work will leverage inverse rendering and integrate physics engines, such as Blender, to produce assets with exceptional realism and precision. Furthermore, combining large language models (LLMs), vision-language models (VLMs), and agent-based symbolic frameworks will enable a flexible pipeline for asset generation, significantly reducing manual labor and enhancing creative workflows in industries like gaming and filmmaking.
        <br>
        <b>The Final Goal and Vision: Interactive Dual-Engine-Driven World Simulator and AIGC System</b>
        By synthesizing these three sub-goals, my research aims to create a dual-engine-driven simulator that merges generative diversity with structural precision. This simulator will support dynamic, interactive, and highly realistic virtual environments, offering scalable solutions for applications in gaming, virtual testing, robotics, and AI-driven simulations. Ultimately, this work aspires to push the boundaries of interactive world simulation, driving both academic advancements and transformative real-world innovations.
        <br>
        <br>
       <img src="images/researchstatement.png" alt="Life Illustration" style="display:block;margin:0 auto;" width="100%" height="100%">

      </td>
    </tr>
  </tbody>
</table>


	      	
	      
	      <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5ecx1b29xzl&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>



                      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                        <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                          <heading style="font-size: 26px;"><i>Research Experience</i></heading>
                        </td>
                      </tr>
                    </tbody></table>
              
                    <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      
                      <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                          <img src="images/stanford_university.png" alt="layoutgpt_gif" width="100" height="100 " style="border-style: none">
                        </td>
                        <td width="75%" valign="middle">
                          <b><a href="https://svl.stanford.edu/"> Stanford AI Lab</a>, Stanford University </b>, Research Intern &emsp;  06/24 ‚Äì 08/24
                          <br>
       
                          Generative Spatial Intelligence.              
                          <br>
                          <br>
                        </tr>
      

                        <tr>
                          <td style="padding:20px;width:25%;vertical-align:middle">
                            <img src="images/OPPO.png" alt="layoutgpt_gif" width="100" height="75 " style="border-style: none">
                          </td>
                          
                          <td width="75%" valign="middle">
                            <b>Westlake University & OPPO Research Institude </b>, Research Intern &emsp;  09/23 ‚Äì present 
                            <br>

                            Text driven Video Generation &ensp; advised by <a href="http://maple-lab.net/about.html"> Prof. Guo-Jun Qi (IEEE Fellow) </a>.              
                            <br>
                            <br>
                          </tr>


                          <tr>
                            <td style="padding:20px;width:25%;vertical-align:middle">
                              <img src="images/scut.png" alt="layoutgpt_gif" width="100" height="100 " style="border-style: none">
                            </td>
                            <td width="75%" valign="middle">
                              <b> School of Future Technology, SCUT </b>, Research Intern &emsp;  12/22 ‚Äì present 
                                            <br>
                                            
                                          Text driven Image Generation &ensp; advised by <a href="https://drliuqi.github.io/"> Prof. Qi Liu (IEEE Senior Member) </a>.
                            
                              <br>
                              <br>
                            </tr>

                      </tr>
                      </tbody>


	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>Education Experience</i></heading>
            <p style="font-size: 16px;">
              <b>South China University of Technology (SCUT)</b>, Guangzhou, China &emsp;  09/21 ‚Äì 06/25(expected)  
              <br>
              
              B.Eng &ensp; (Majoring in Artificial Intelligence)
              
              <br>
	      <!-- <br> -->
              <!-- <b>GPA:</b> 3.72 / 4.0 -->
              <!-- <br> -->
	      <br>
              <b>Main courses:</b> Deep Learning and Computer Vision<b>(4.0/4.0)</b>,&ensp; Course Design of Deep Learning and Computer Vision <b>(4.0/4.0,&ensp; Best project</b>),&ensp; 
              C++ Programming Foundations <b>(4.0/4.0)</b>,&ensp; Python Programming <b>(4.0/4.0)</b>,&ensp; Data Structure <b>(4.0/4.0)</b>,&ensp; 
              Advanced Language Programming Training <b>(4.0/4.0)</b>,&ensp; Artificial Intelligence and 3D Vision<b>(4.0/4.0)</b>,&ensp; Calculus <b>(4.0/4.0)</b>......

            </p>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>News</i></heading>
            <p style="font-size: 16px;">
              
              One paper is accepted by AAAI 2024            <!-- While visual inputs have played a more important part, I believe that the two modalities are equally important that work at different levels of abstraction. -->
            </p>
            <p style="font-size: 16px;">
              One paper is accepted by VDU@CVPR 2024 as Oral Presentation           <!-- While visual inputs have played a more important part, I believe that the two modalities are equally important that work at different levels of abstraction. -->
            </p>

            <p style="font-size: 16px;">
               One paper is accepted by IJCAI 2024            <!-- While visual inputs have played a more important part, I believe that the two modalities are equally important that work at different levels of abstraction. -->
            </p>

            <p style="font-size: 16px;">
               One paper is featured as <a href="https://x.com/_akhaliq/status/1821364416488567295"> Hugging Face Daily Papers </a> and reposted by <a href="https://x.com/_akhaliq?t=Xbpfc0mTpJQfRsiC_ugQrw&s=09"> AK </a> .            <!-- While visual inputs have played a more important part, I believe that the two modalities are equally important that work at different levels of abstraction. -->
            </p>

	<p style="font-size: 16px;">
               One paper is accepted by CVPR 2025, See you in Nashville!            <!-- While visual inputs have played a more important part, I believe that the two modalities are equally important that work at different levels of abstraction. -->
            </p>


		  
          </td>
        </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>Publications</i></heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/dynamicscaler.gif" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>DynamicScaler: Seamless and Scalable Video Generation for Panoramic Scenes</papertitle>
            </a>
            <br><br>
            <strong style="color: #0F52BA;">Jinxiu Liu</strong>, &nbsp;
            <a href="" style="color: black;">Shaoheng Lin</a>, &nbsp;
            <a href="" style="color: black;">Yinxiao Li</a>, &nbsp;
            <a href="" style="color: black;">Ming-Hsuan Yang</a>, &nbsp;
		  
            <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
            <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>
            The increasing demand for immersive AR/VR applications and spatial intelligence has heightened the need to generate high-quality scene-level and 360¬∞ panoramic video. However, most video diffusion models are constrained by limited resolution and aspect ratio, which restricts their applicability to scene-level dynamic content synthesis. In this work, we propose the DynamicScaler, addressing these challenges by enabling spatially scalable and panoramic dynamic scene synthesis that preserves coherence across panoramic scenes of arbitrary size. Specifically, we introduce a Offset Shifting Denoiser, facilitating efficient, synchronous, and coherent denoising panoramic dynamic scenes via a diffusion model with fixed resolution through a seamless rotating Window, which ensures seamless boundary transitions and consistency across the entire panoramic space, accommodating varying resolutions and aspect ratios. Additionally, we employ a Global Motion Guidance mechanism to ensure both local detail fidelity and global motion continuity. Extensive experiments demonstrate our method achieves superior content and motion quality in panoramic scene-level video generation, offering a training-free, efficient, and scalable solution for immersive dynamic scene creation with constant VRAM consumption regardless of the output video resolution.<br><br>
            <em style="font-size: 16px;"> CVPR 2025 </em>
            <br>
            <!-- <a href="">paper (Attached by email)</a> /  -->
	    <a href="https://arxiv.org/abs/2412.11100">Paper / </a> 
            <!-- <a href="https://brandon-liu-jx.github.io/r3cd/">project page</a>  -->
	    
            <a href="https://dynamic-scaler.pages.dev/">project page / </a> 
		  
            <a href="https://huggingface.co/papers/2412.11100/">Huggingface Daily Papers</a>
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>
	      
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/r3cdpipeline.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>R3CD: Scene Graph to Image Generation with Relation-aware Compositional
                Contrastive Control Diffusion</papertitle>
            </a>
            <br><br>
            <strong style="color: #0F52BA;">Jinxiu Liu</strong>, &nbsp;
            <a href="https://drliuqi.github.io/" style="color: black;">Qi Liu</a>, &nbsp;
            <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
            <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>
            In this paper, we introduce R3CD, a new image generation framework from <b>scene graphs</b> with large-scale diffusion models and contrastive control mechanisms. 
            R3CD can handle <b> complex or ambiguous relations</b> in <b>complex multi-object scene graphs</b> and produce realistic and diverse images that match the scene graph specifications. 
            R3CD consists of two main components: (1) SGFormer, a transformer-based encoder that <b>captures both local and global information from scene graphs</b>; (2) Relation-aware Diffusion contrastive control, a <b>contrastive learning</b> module that <b>aligns the relation features and the image features across different levels of abstraction</b>.
            <br><br>
            <em style="font-size: 16px;">AAAI 2024, <font color="#FF0000">4 positive reviews</font></em>
            <br>
            <!-- <a href="">paper (Attached by email)</a> /  -->
	    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28155">Paper</a> 
            <!-- <a href="https://brandon-liu-jx.github.io/r3cd/">project page</a>  -->
	    
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/nips_firstpage.png" alt="layoutgpt_gif" width="210" height="140 " style="border-style: none">
<!--             <img src="images/fire.png" alt="layoutgpt_gif" width="150" height="150 " style="border-style: none"> -->

          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>Maple: Multi-modal Pre-training for Contextual Instance-aware Visual Generation</papertitle>
            </a>
            <br><br>
            <strong style="color: #0F52BA;">Jinxiu Liu</strong>, &nbsp;
            <a href="" style="color: black;">Jinjin Cao</a>, &nbsp;
            <a href="" style="color: black;">Zilyu Ye</a>, &nbsp;
            <a href="" style="color: black;">Zhiyang Chen</a>, &nbsp;
            <a href="" style="color: black;">Ziwei Xuan</a>, &nbsp;
            <a href="" style="color: black;">Zemin Huang</a>, &nbsp;
            <a href="" style="color: black;">Mingyuan Zhou</a>, &nbsp;
            <a href="" style="color: black;">Xiaoqian Shen</a>, &nbsp;
            <a href="" style="color: black;">Qi Liu</a>, &nbsp;
            <a href="" style="color: black;">Mohamed Elhoseiny </a>, &nbsp;
            <a href="" style="color: black;">Guo-Jun Qi</a>  &nbsp;
        
            <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
            <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>
	Despite the high quality in images generated with short captions, most of them deteriorate drastically when <b>long contexts with multiple instances</b> are provided. 
		  The resulting instances in the generated images are hard to be consistent with those in former images.
		  To address this issue, we propose Maple, 
		  a large-scale open-domain generative multi-modal model. 
		  Maple is able to take long context with interleaved images and text as guidance to generate images and keep the instances in the generated image consistent with the given inputs. 
		  In this paper, we propose a training schedule focused on instance-level consistency, and a long-context decoupling mechanism to balance contextual information of different importance. 
				   <br>
		   <br>
		  <em style="font-size: 16px;">Tech Report, MLLM Pretraining for Interleaved Image-text Generation based on Openstory++ </em>
            <br>
<!--             <a href="https://www.cs.rochester.edu/u/yyao39/files/PiLife.pdf">Paper</a> /
            <a href="https://promptimage2life.github.io/">project page</a>  -->
            <p></p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/text2video_A_man_is_speaking_loudly.mp4 (1).gif" alt="layoutgpt_gif" width="150" height="150 " style="border-style: none">
            <img src="images/fire.png" alt="layoutgpt_gif" width="150" height="150 " style="border-style: none">

          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle><font color="#FF0000">P</font>rompt <font color="#FF0000">i</font>mage to <font color="#FF0000">Life</font>: Training-free Text-driven Image-to-video Generation</papertitle>
            </a>
            <br><br>
            <strong style="color: #0F52BA;">Jinxiu Liu</strong>, &nbsp;
            <a href="" style="color: black;">Yuan Yao</a>, &nbsp;
            <a href="" style="color: black;">Bingwen Zhu</a>, &nbsp;
            <a href="" style="color: black;">Fanyi Wang</a>, &nbsp;
            <a href="" style="color: black;">Weijian Luo</a>, &nbsp;
            <a href="" style="color: black;">Jingwen Su</a>, &nbsp;
            <a href="" style="color: black;">Yanhao Zhang</a>, &nbsp;
            <a href="" style="color: black;">Yuxiao Wang</a>, &nbsp;
            <a href="" style="color: black;">Liyuan Ma</a>, &nbsp;
            <a href="" style="color: black;">Qi Liu</a>, &nbsp;
            <a href="" style="color: black;">Jiebo Luo</a>, &nbsp;
            <a href="" style="color: black;">Guo-Jun Qi</a>  &nbsp;
        
            <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
            <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>

          We propose PiLife, a novel training-free I2V framework that only leverages a pre-trained text-to-image diffusion model. PiLife can generate videos that are coherent with a given image and aligned with the semantics of a given text, which mainly consists of three components: (i) A motion-aware diffusion inversion module that embeds motion semantics into the inverted images as the initial frames; (ii) A motion-aware noise initialization module that employs a motion text attention map to modulate the diffusion process and adjust the motion intensity of different regions with spatial noise;  (iii) A probabilistic cross-frame attention module that leverages a geometric distribution to randomly sample a frame and compute attention with it, thereby enhancing the motion diversity. Experiments show that PiLife significantly outperforms the training-free baselines, and is comparable or even superior to some training-based I2V methods.         <br><br>
            <em style="font-size: 16px;">Tech Report, Best Project in "MetaVerse and VR Course Project"</em>
            <br>
            <a href="https://www.cs.rochester.edu/u/yyao39/files/PiLife.pdf">Paper</a> /
            <a href="https://promptimage2life.github.io/">project page</a> 
            <p></p>
          </td>
        </tr>

	      	      
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/OpenStory++.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>Openstory++: A Large-scale Dataset and Benchmark for Instance-aware Open-domain Visual Storytelling</papertitle>
            </a>
            <br><br>
            <a href="https://yeluosuiyou.github.io/" style="color: black;">Zilyu Ye *</a>, &nbsp;
            <strong style="color: #0F52BA;">Jinxiu Liu * &Dagger; </strong>, &nbsp;
		<a href="" style="color: black;">Ruotian Peng *</a>  &nbsp;  
            <a href="" style="color: black;">Jinjin Cao</a>  &nbsp;
            <a href="" style="color: black;">Zhiyang Chen</a>  &nbsp;
            <a href="" style="color: black;">Ziwei Xuan</a>  &nbsp;
            <a href="" style="color: black;">Mingyuan Zhou</a>  &nbsp;
            <a href="" style="color: black;">Xiaoqian Shen</a>  &nbsp;
            <a href="" style="color: black;">Mohamed Elhoseiny</a>  &nbsp;
            <a href="https://drliuqi.github.io/" style="color: black;">Qi Liu</a>, &nbsp;
            <a href="" style="color: black;">Guo-Jun Qi</a>  &nbsp; &nbsp (* contribute equally, &Dagger; Project Lead)
            <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
            <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>
            Recent image generation models excel at creating high-quality images from brief captions. 
		  However, they fail to maintain consistency of multiple instances across images when encountering lengthy contexts. 
		  This inconsistency is largely due to in existing training datasets the absence of granular instance feature labeling in existing training datasets. 
		  To tackle these issues, we introduce Openstory++, a large-scale dataset combining additional instance-level annotations with both images and text. 
		  Furthermore, we develop a training methodology that emphasizes entity-centric image-text generation, ensuring that the models learn to effectively interweave visual and textual information.
		  Specifically, Openstory++ streamlines the process of keyframe extraction from open-domain videos, 
		  employing vision-language models to generate captions that are then polished by a large language model for narrative continuity. 
		  It surpasses previous datasets by offering a more expansive open-domain resource, which incorporates automated captioning, 
		  high-resolution imagery tailored for instance count, and extensive frame sequences for temporal consistency. 
		  Additionally, we present Cohere-Bench, a pioneering benchmark framework for evaluating the image generation tasks when long multimodal context is provided, 
		  including the ability to keep the background, style, instances in the given context coherent.
		  Compared to existing benchmarks, our work fills critical gaps in multi-modal generation, 
		  propelling the development of models that can adeptly generate and interpret complex narratives in open-domain environments. 
		  Experiments conducted within Cohere-Bench confirm the superiority of Openstory++ in nurturing high-quality visual storytelling models, 
		  enhancing their ability to address open-domain generation tasks.
                        
            <br><br>
		 
            <em style="font-size: 16px;">Tech Report,  <font color="#FF0000">üèÜ Hugging Face Daily Papers</font></em>
<br>
		   <a href="https://arxiv.org/pdf/2408.03695">Paper</a> /
		  <a href="https://openstorypp.github.io/">Project Page</a> /
<!-- 		  <a href="https://arxiv.org/pdf/2408.03695">Dataset</a> -->
		  
            <br>

            <p></p>
          </td>
        </tr>
	      
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/OpenStory.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>OpenStory: A Large-Scale Open-Domain Dataset for Subject-Driven Visual Storytelling</papertitle>
            </a>
            <br><br>
            <a href="https://yeluosuiyou.github.io/" style="color: black;">Zilyu Ye *</a>, &nbsp;
            <strong style="color: #0F52BA;">Jinxiu Liu * &Dagger; </strong>, &nbsp;
            <a href="" style="color: black;">Jinjin Cao</a>  &nbsp;
            <a href="" style="color: black;">Zhiyang Chen</a>  &nbsp;
            <a href="" style="color: black;">Ziwei Xuan</a>  &nbsp;
            <a href="" style="color: black;">Mingyuan Zhou</a>  &nbsp;
            <a href="https://drliuqi.github.io/" style="color: black;">Qi Liu</a>, &nbsp;
            <a href="" style="color: black;">Guo-Jun Qi</a>  &nbsp; &nbsp (* contribute equally, &Dagger; Project Lead)
            <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
            <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>
            In this paper, we present OpenStory, a large-scale dataset tailored for training subject-focused story visualization models to
             generate coherent and contextually relevant visual narratives. Addressing the challenges of maintaining subject continuityacross frames
              and capturing compelling narratives, We propose an innovative pipeline that automates the extraction ofkeyframes from open-domain videos.
               It ingeniously employsvision-language models to generate descriptive captions,which are then refined by a large language model to
                ensurenarrative flow and coherence. Furthermore, advanced sub-ject masking techniques are applied to isolate and segmentthe primary subjects. 
                Derived from diverse video sources,including YouTube and existing datasets, OpenStory offersa comprehensive open-domain resource, surpassing priordatasets confined to 
                specific scenarios. With automatedcaptioning instead of manual annotation, high-resolutionimagery optimized for subject count per frame, and exten-sive frame sequences
                 ensuring consistent subjects for tem-poral modeling, OpenStory establishes itself as an invalu-able benchmark. It facilitates advancements in subject-focused story visualization,
                  enabling the training of modelscapable of comprehending and generating intricate multi-modal narratives from extensive visual and textual inputs.
                        
            <br><br>
		 
            <em style="font-size: 16px;">CVPR 2024@VDU, <font color="#FF0000">Oral Presentation</font></em>
<br>
		   <a href="https://openaccess.thecvf.com/content/CVPR2024W/VDU/html/Ye_OpenStory_A_Large-Scale_Open-Domain_Dataset_for_Subject-Driven_Visual_Storytelling_CVPRW_2024_paper.html">Paper</a> /
		  
            <br>

            <p></p>
          </td>
        </tr>




        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/PiGIE.png" alt="layoutgpt_gif" width="200" height="310 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>üêñPiGIE: Proximal Policy Optimization Guided Diffusion for Fine-Grained Image Editing</papertitle>
            </a>
            <br><br>
            <a href="" style="color: black;">Tiancheng Li*</a>, &nbsp;
            <strong style="color: #0F52BA;">Jinxiu Liu* &Dagger;</strong>, &nbsp;
            <a href="" style="color: black;"> William Luo</a>, &nbsp;     
            <a href="" style="color: black;"> Huajun Chen</a>, &nbsp; 
            <a href="" style="color: black;">Qi Liu</a>, &nbsp;
            <b>(* contribute equallyÔºå &Dagger; Project Lead)</b>
            <br>
            <br>
            Instruction-based image editing is a challenging task since it requires to manipulation of the visual content of images according to complex human language instructions. 
            When editing an image with tiny objects and complex positional relationships, existing image editing methods cannot locate the accurate region to execute the editing. 
            To address this issue, we introduce Proximal Policy Optimization Guided Image Editing(PiGIE), a diffusion model that can accurately edit tiny objects in images with complex scenes. 
            The PiGIE is able to incorporate proper noise masks to edit images based on the guidance of the target object‚Äôs attention maps. 
            Different from the traditional image editing approaches based on supervised learning, PiGIE leverages the cosine similarity between UNet‚Äôs attention map 
            and simulated human feedback as a reward function and employs Proximal Policy Optimization (PPO) to fine-tune the diffusion model, such that PiGIE can locate the editing 
            regions precisely based on human instructions. On multiple image editing benchmarks, PiGIE exhibits remarkable improvements in both            
            mage quality and generalization capability. In particular, PiGIE sets a new baseline for editing fine-grained images with multiple tiny objects, 
            shedding light on future studies on text-guided image editing for tiny objects.
            <br>
            <br>
            <em style="font-size: 16px;">AI4CC@CVPR 2024</em>
            <br>
            <a href="https://arxiv.org/abs/2406.09973">Extended Paper</a> /
		  <a href="https://arxiv.org/abs/2406.09973">Original Paper</a>
            <!-- <a href="">paper (Attached by email)</a> /  -->
            <!-- <a href="https://layoutgpt.github.io">project page</a>  -->
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>

	      
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/pose_cai.gif" alt="layoutgpt_gif" width="150" height="150 " style="border-style: none">
            <img src="images/video_cai.gif" alt="layoutgpt_gif" width="150" height="150 " style="border-style: none">

          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>PoseAnimate: Zero-shot High Fidelity Pose Controllable Character Animation</papertitle>
            </a>
            <br><br>
            <a href="" style="color: black;">Bingwen Zhu</a>, &nbsp;
            <a href="" style="color: black;">Fanyi Wang</a>, &nbsp;
            <a href="" style="color: black;"> Peng Liu</a>, &nbsp;     
            <a href="" style="color: black;"> Jingwen Su</a>, &nbsp; 
            <strong style="color: #0F52BA;">Jinxiu Liu</strong>, &nbsp;
            <a href="" style="color: black;">Yanhao Zhang</a>, &nbsp;
            <a href="" style="color: black;">Zuxuan Wu</a>, &nbsp;
            <a href="" style="color: black;"> Yu-Gang Jiang</a>, &nbsp;            
            <a href="" style="color: black;">Guo-Jun Qi</a>, &nbsp;

            <br>
            <br>
            In this paper, we propose PoseAnimate, a novel zero-shot I2V framework for character animation.PoseAnimate contains 
            three key components: 1) Pose-Aware Control Module (PACM) incorporates diverse pose signals into conditional embeddings, 
            to preserve character-independent content and maintain precise alignment of actions.2) Dual Consistency Attention Module 
            (DCAM) enhances temporal consistency, and retains character identity and intricate background details.3) Mask-Guided 
            Decoupling Module (MGDM) refines distinct feature perception, improving animation fidelity by decoupling the character 
            and background.We also propose a Pose Alignment Transition Algorithm (PATA) to ensure smooth action transition.Extensive 
            experiment results demonstrate that our approach 
            outperforms the state-of-the-art training-based methods in terms of character consistency and detail fidelity.           
             <br>
             <br>
             <em style="font-size: 16px;">IJCAI 2024</em>
            <br>
            <a href="https://arxiv.org/pdf/2404.13680.pdf">Paper</a> /
            <a href="https://poseanimate.github.io/">project page</a> 
            <!-- <a href="">paper (Attached by email)</a> /  -->
            <!-- <a href="https://layoutgpt.github.io">project page</a>  -->
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>


        <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SLR.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>Deep Neural Network Compression by Spatial-wise Low-rank
                  Decomposition</papertitle>
              </a>
              <br><br>
              <a href="https://github.com/Xshellye" style="color: black;">Xiaoye Zhu*</a>, &nbsp;
              <strong style="color: #0F52BA;">Jinxiu Liu*</strong>, &nbsp;
              <a href="" style="color: black;">Ye Liu</a>, &nbsp;
              <a href=" https://hkumath.hku.hk/MathWWW/people.php?faculty.mng" style="color: black;">Michael NG</a>, &nbsp;
              <a href="" style="color: black;">Zihan Ji</a>, &nbsp;
              <b>(* contribute equally)</b>
             
              <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
              <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
              <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
              <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
              <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
              <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
              <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
              <br>
              <br>
              In this paper, we introduces a new method for compressing convolutional neural networks (CNNs) based on spatial-wise low-rank decomposition (SLR). The method preserves the higher-order structure of the filter weights and exploits their local low-rankness in different spatial resolutions, which can be implemented as a 1x1 convolution layer and achieves significant reductions in model size and computation cost with minimal accuracy loss. The paper shows the superior performance of the method over state-of-the-art low-rank compression methods and network pruning methods on several popular CNNs and datasets. 
              <br>              
              <br>
              <em style="font-size: 16px;">Best Project (1/96) in "Optimization Method Course Project"</em>
              <br>
              <!-- <a href="">paper (Attached by email)</a> /  -->
              <!-- <a href="https://layoutgpt.github.io">project page</a>  -->
              <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
              <p></p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>Projects</i></heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="r3cd/img/huggingface.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2305.15393">
              <papertitle>MiniHuggingGPTÔºöA mini multi-modal application like HuggingGPT and MiniGPT-4</papertitle>
            </a>
            <br><br>
            Course Design of Deep Learning and Computer Vision</strong>&nbsp mentored by 
      
            <a href="https://tanmingkui.github.io/" style="color:#0F52BA;">Prof. Mingkui Tan and</a>
            <a href="https://zhuanghp.github.io/" style="color:#0F52BA;">Prof. Huiping Zhuang</a>, &nbsp;
            <!-- <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>
            - <b>Took LLM as an API</b> that calls other large-scale models based on <b>natural language instructions</b>.
            <br>
            - Developed a text-based dialogue system based on ChatGLM that can call for three large-scale models for image captioning, image generation, and text conversation using natural language commands by <b>instruction finetuning</b>.
            <br>
            - <b>Provided a webui interface based on gradio</b> for easy interaction with the system and showcased various examples of its capabilities.            <br><br>
            <em style="font-size: 16px;"><b>Awarded as the Best Course Design, 1/39
            </b></em>
            <br>
            <br>
            <!-- <a href="https://arxiv.org/abs/2305.15393">arxiv</a> /  -->
            <a href="https://github.com/Brandon-Liu-Jx/ChatGLM-based-multi-modal-web-UI/blob/main/project_report.pdf">project report</a> 
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>Talk</i></heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/fewshot.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>Introduction to few-shot learning</papertitle>
            </a>
            <br><br>
           <i>Honored to be invited by <a href="https://xinjie-shen.com/">Xinjie Shen</a>, Chairman of AIA(Artificial Intelligence Association) in SCUT.
           </i><br>
            <br>
            In this talk, I explore the use of few-shot learning techniques for RE based on my research experience. I introduce the basic concepts and principles of few-shot learning, such as the meta-learning framework, the episodic training strategy, and the evaluation metrics. I also discuss some recent advances and applications of few-shot learning for RE, such as the use of pre-trained language models, graph neural networks, contrastive learning, and data augmentation. I demonstrate how few-shot learning can improve the performance and robustness of RE models on different datasets and scenarios. I also share some of the challenges and future directions of few-shot learning for RE.
  
          </em>
          <br>
            <br>
            <!-- <a href="https://arxiv.org/abs/2305.15393">arxiv</a> /  -->
            <a href="assets/slides1.pdf">Slides</a> 
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>Awards</i></heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>  
       <td> </td>
       <td> </td>
       <td> </td>
      </tbody></table> 




      
      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading style="font-size: 26px;">Service</heading>
          <p style="font-size: 16px;">
            <li>Reviewer: EACL 2023, ACL 2023, NeurIPS 2023</li>
          </p>
        </td>
      </tr>
      </tbody></table> -->



      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading style="font-size: 26px;">Teaching</heading>
          <p style="font-size: 16px;">
            <li>CS165B Machine Learning, 2020-2021, Spring 2022</li>
            <li>ECE239 Deep Learning, Winter 2019</li>
          </p>
        </td>
      </tr>
      </tbody></table> -->


      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Misc.</heading>
          <p>
            <li>Hong Kong SAR Government Scholarship, 2016-2018</li>
          </p>
        </td>
      </tr>
      </tbody></table> -->

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service & Teaching</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">Reviewer for </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
					
        </tbody></table> -->

    </br>
    <div align="center">
      <!-- <a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/count2/yZuS/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a></div> -->
      <!-- <a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/map/yZuS/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a></div> -->
      <a href="https://info.flagcounter.com/2WEU"><img src="https://s11.flagcounter.com/count2/2WEU/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
  </div>
  </br>

  <div align="center">

    <body>
      <font color="gray">&copy Brandon Jinxiu Liu</font>
    </body>
  </div>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-VVQP7QG5B7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VVQP7QG5B7');
</script>
	  



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template from  <a href="https://jonbarron.info/">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
