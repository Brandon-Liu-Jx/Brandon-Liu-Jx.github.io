<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jinxiu Liu</title>
  
  <meta name="author" content="Jinxiu Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><i>Brandon Jinxiu Liu (ÂàòÈî¶Áª£)</i></name>
              </p>
              <p style="font-size: 16px;"> <b><i>‚ÄúStay hungry, Stay foolish‚Äù -- Steven Jobs</i> </b>
</p>
              
              <p style="font-size: 16px;">Hi there! I am a junior undergraduate student from South China University of Technology, advised by <a href="https://drliuqi.github.io/"> Prof. Qi Liu (IEEE Senior Member) </a>. Now I am working as a research intern 
                at <a href="https://svl.stanford.edu/"> Stanford Vision and Learning Lab</a>, 
                focusing on <b>4D Dynamic Generation</b>, advised by <a href="https://jiajunwu.com/"> Prof. Jiajun Wu </a> .
                 I am also working at Westlake University & OPPO Research Institude, focusing on Multi-modal LLM enhanced <b>Diffusion Model based Image/Video Generation</b>, advised by <a href="http://maple-lab.net/about.html"> Prof. Guo-Jun Qi (IEEE Fellow) </a>.
                                <br>

                <br>
               <!-- I have prior research experience in NLP and CV as well, working with <a href="https://ziqianzeng.github.io/">Prof. Ziqian Zeng </a> and <a href="https://www2.scut.edu.cn/ft/2021/1102/c29779a449612/page.htm"> Prof. Ye Liu</a> . -->


            <font color="#FF0000">Sincerely looking for PhD positions for fall 2025 admission!    </font>

   <br>      </p>
              <p style="font-size: 16px;">
                Email: jinxiuliu0628@foxmail.com &nbsp/&nbsp branodnjinxiuliu@cs.stanford.edu
                <br>
                Tel/Wechat: +86-13951891694
              </p>
              <p style="text-align:center;">
                <a href="mailto:branodnjinxiuliu@cs.stanford.edu">Email</a> &nbsp/&nbsp
                <a href="assets/Jinxiu_Liu_resume.pdf">CV</a> 
                &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=HsZpy3gAAAAJ&hl=zh-CN">Google Scholar</a> 
                <!-- <a href="https://github.com/Brandon-Liu-Jx">Github</a> &nbsp/&nbsp -->
                <!-- <a href="https://twitter.com/weixi_feng">Twitter</a> &nbsp/&nbsp -->

              </p>
            </td>

            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="javascript:void(0)" onclick="var img = this.children[0]; if (img.tagName === 'IMG' && img.src.includes('life.jpg')) { img.src = 'images/brandon_img.gif'; } else if (img.tagName === 'IMG' && img.src.includes('brandon_img.gif')) { img.src = 'images/life.jpg'; }">
                  <img style="width:100%;max-width:100%" alt="profile photo" src="images/life.jpg" class="hoverZoomLink">
                  
              </a>
              <br>
              <font color="#FF0000">Sidelights!!!</font> Click the portrait üëÜ and enjoy the animation magic from my project <b><font color="#FF0000">P</font>rompt <font color="#FF0000">i</font>mage to <font color="#FF0000">Life</font></b>.
          </td>
          
            
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>Education Experience</i></heading>
            <p style="font-size: 16px;">
              <b>South China University of Technology (SCUT)</b>, Guangzhou, China &emsp;  09/21 ‚Äì 06/25(expected)  
              <br>
              
              B.Eng &ensp; (Majoring in Artificial Intelligence)
              
              <br>
	      <!-- <br> -->
              <!-- <b>GPA:</b> 3.72 / 4.0 -->
              <!-- <br> -->
	      <br>
              <b>Main courses:</b> Deep Learning and Computer Vision<b>(4.0/4.0)</b>,&ensp; Course Design of Deep Learning and Computer Vision <b>(4.0/4.0,&ensp; Best project</b>),&ensp; 
              C++ Programming Foundations <b>(4.0/4.0)</b>,&ensp; Python Programming <b>(4.0/4.0)</b>,&ensp; Data Structure <b>(4.0/4.0)</b>,&ensp; 
              Advanced Language Programming Training <b>(4.0/4.0)</b>,&ensp; Artificial Intelligence and 3D Vision<b>(4.0/4.0)</b>,&ensp; Calculus <b>(4.0/4.0)</b>......

            </p>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>News</i></heading>
            <p style="font-size: 16px;">
              
              One paper is accepted by AAAI 2024            <!-- While visual inputs have played a more important part, I believe that the two modalities are equally important that work at different levels of abstraction. -->
            </p>
            <p style="font-size: 16px;">
              One paper is accepted by VDU@CVPR 2024 as Oral Presentation           <!-- While visual inputs have played a more important part, I believe that the two modalities are equally important that work at different levels of abstraction. -->
            </p>

            <p style="font-size: 16px;">
               One paper is accepted by IJCAI 2024            <!-- While visual inputs have played a more important part, I believe that the two modalities are equally important that work at different levels of abstraction. -->
            </p>

          </td>
        </tr>
        </tbody></table>

        







                      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                        <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                          <heading style="font-size: 26px;"><i>Research Experience</i></heading>
                        </td>
                      </tr>
                    </tbody></table>
              
                    <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      
                      <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                          <img src="images/stanford_university.png" alt="layoutgpt_gif" width="100" height="100 " style="border-style: none">
                        </td>
                        <td width="75%" valign="middle">
                          <b><a href="https://svl.stanford.edu/"> Stanford Vision and Learning Lab</a>, Stanford University </b>, Research Intern &emsp;  03/24 ‚Äì present 
                          <br>
       
                          4D Scene Generation &ensp; advised by <a href="https://jiajunwu.com/"> Prof. Jiajun Wu</a> and <a href="https://kovenyu.com/">Hong-Xing "Koven" Yu</a>.              
                          <br>
                          <br>
                        </tr>
      

                        <tr>
                          <td style="padding:20px;width:25%;vertical-align:middle">
                            <img src="images/OPPO.png" alt="layoutgpt_gif" width="100" height="75 " style="border-style: none">
                          </td>
                          
                          <td width="75%" valign="middle">
                            <b>Westlake University & OPPO Research Institude </b>, Research Intern &emsp;  09/23 ‚Äì present 
                            <br>

                            Text driven Video Generation &ensp; advised by <a href="http://maple-lab.net/about.html"> Prof. Guo-Jun Qi (IEEE Fellow) </a>.              
                            <br>
                            <br>
                          </tr>


                          <tr>
                            <td style="padding:20px;width:25%;vertical-align:middle">
                              <img src="images/scut.png" alt="layoutgpt_gif" width="100" height="100 " style="border-style: none">
                            </td>
                            <td width="75%" valign="middle">
                              <b> School of Future Technology, SCUT </b>, Research Intern &emsp;  12/22 ‚Äì present 
                                            <br>
                                            
                                          Text driven Image Generation &ensp; advised by <a href="https://drliuqi.github.io/"> Prof. Qi Liu (IEEE Senior Member) </a>.
                            
                              <br>
                              <br>
                            </tr>

                      </tr>
                      </tbody>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>Publication</i></heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/r3cdpipeline.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>R3CD: Scene Graph to Image Generation with Relation-aware Compositional
                Contrastive Control Diffusion</papertitle>
            </a>
            <br><br>
            <strong style="color: #0F52BA;">Jinxiu Liu</strong>, &nbsp;
            <a href="https://drliuqi.github.io/" style="color: black;">Qi Liu</a>, &nbsp;
            <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
            <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>
            In this paper, we introduce R3CD, a new image generation framework from scene graphs with large-scale diffusion models and contrastive control mechanisms. 
            R3CD can handle complex or ambiguous relations in scene graphs and produce realistic and diverse images that match the scene graph specifications. 
            R3CD consists of two main components: (1) SGFormer, a transformer-based encoder that <b>captures both local and global information from scene graphs</b>; (2) Relation-aware Diffusion contrastive control, a <b>contrastive learning</b> module that <b>aligns the relation features and the image features across different levels of abstraction</b>.
            <br><br>
            <em style="font-size: 16px;">AAAI 2024, <font color="#FF0000">4 positive reviews</font></em>
            <br>
            <!-- <a href="">paper (Attached by email)</a> /  -->
	    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28155">Paper</a> 
            <!-- <a href="https://brandon-liu-jx.github.io/r3cd/">project page</a>  -->
	    
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/OpenStory.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>OpenStory: A Large-Scale Open-Domain Dataset for Subject-Driven Visual Storytelling</papertitle>
            </a>
            <br><br>
            <a href="https://yeluosuiyou.github.io/" style="color: black;">Zilyu Ye *</a>, &nbsp;
            <strong style="color: #0F52BA;">Jinxiu Liu *</strong>, &nbsp;
            <a href="" style="color: black;">Jinjin Cao</a>  &nbsp;
            <a href="" style="color: black;">Zhiyang Chen</a>  &nbsp;
            <a href="" style="color: black;">Ziwei Xuan</a>  &nbsp;
            <a href="" style="color: black;">Mingyuan Zhou</a>  &nbsp;
            <a href="https://drliuqi.github.io/" style="color: black;">Qi Liu</a>, &nbsp;
            <a href="" style="color: black;">Guo-Jun Qi</a>  &nbsp; &nbsp (* contribute equally)
            <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
            <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>
            In this paper, we present OpenStory, a large-scale dataset tailored for training subject-focused story visualization models to
             generate coherent and contextually relevant visual narratives. Addressing the challenges of maintaining subject continuityacross frames
              and capturing compelling narratives, We propose an innovative pipeline that automates the extraction ofkeyframes from open-domain videos.
               It ingeniously employsvision-language models to generate descriptive captions,which are then refined by a large language model to
                ensurenarrative flow and coherence. Furthermore, advanced sub-ject masking techniques are applied to isolate and segmentthe primary subjects. 
                Derived from diverse video sources,including YouTube and existing datasets, OpenStory offersa comprehensive open-domain resource, surpassing priordatasets confined to 
                specific scenarios. With automatedcaptioning instead of manual annotation, high-resolutionimagery optimized for subject count per frame, and exten-sive frame sequences
                 ensuring consistent subjects for tem-poral modeling, OpenStory establishes itself as an invalu-able benchmark. It facilitates advancements in subject-focused story visualization,
                  enabling the training of modelscapable of comprehending and generating intricate multi-modal narratives from extensive visual and textual inputs.
                        
            <br><br>
            <em style="font-size: 16px;">CVPR 2024@VDU, <font color="#FF0000">Oral Presentation</font></em>
            <br>

            <p></p>
          </td>
        </tr>



        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/text2video_A_man_is_speaking_loudly.mp4 (1).gif" alt="layoutgpt_gif" width="150" height="150 " style="border-style: none">
            <img src="images/fire.png" alt="layoutgpt_gif" width="150" height="150 " style="border-style: none">

          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle><font color="#FF0000">P</font>rompt <font color="#FF0000">i</font>mage to <font color="#FF0000">Life</font>: Training-free Text-driven Image-to-video Generation</papertitle>
            </a>
            <br><br>
            <strong style="color: #0F52BA;">Jinxiu Liu</strong>, &nbsp;
            <a href="" style="color: black;">Yuan Yao</a>, &nbsp;
            <a href="" style="color: black;">Bingwen Zhu</a>, &nbsp;
            <a href="" style="color: black;">Fanyi Wang</a>, &nbsp;
            <a href="" style="color: black;">Weijian Luo</a>, &nbsp;
            <a href="" style="color: black;">Jingwen Su</a>, &nbsp;
            <a href="" style="color: black;">Yanhao Zhang</a>, &nbsp;
            <a href="" style="color: black;">Yuxiao Wang</a>, &nbsp;
            <a href="" style="color: black;">Liyuan Ma</a>, &nbsp;
            <a href="" style="color: black;">Qi Liu</a>, &nbsp;
            <a href="" style="color: black;">Jiebo Luo</a>, &nbsp;
            <a href="" style="color: black;">Guo-Jun Qi</a>  &nbsp;
        
            <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
            <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>
            Image-to-video (I2V) generation is a challenging task that requires transforming a static image into a dynamic video according to a text prompt. 
            For a long time, it has been a challenging task that demands both subject consistency and text semantic alignment. 
            Moreover, existing I2V generators require expensive training on large video datasets. 
            To address this issue, we propose PiLife, a novel training-free I2V framework that leverages a pre-trained text-to-image diffusion model. PiLife can generate videos that are coherent with a given image and aligned with the semantics of a given text, which mainly consists of three components: (i) A motion-aware diffusion inversion module that embeds motion semantics into the inverted images as the initial frames; (ii) A motion-aware noise initialization module that employs a motion text attention map to modulate the diffusion process and adjust the motion intensity of different regions with spatial noise;  (iii) A probabilistic cross-frame attention module that leverages a geometric distribution to randomly sample a frame and compute attention with it, thereby enhancing the motion diversity. Experiments show that PiLife significantly outperforms the training-free baselines, and is comparable or even superior to some training-based I2V methods.         <br><br>
            <em style="font-size: 16px;">ECCV 2024, under review</em>
            <br>
            <a href="https://www.cs.rochester.edu/u/yyao39/files/PiLife.pdf">Paper</a> /
            <a href="https://promptimage2life.github.io/">project page</a> 
            <p></p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/pose.gif" alt="layoutgpt_gif" width="150" height="150 " style="border-style: none">
            <img src="images/00011.gif" alt="layoutgpt_gif" width="150" height="150 " style="border-style: none">

          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>PoseAnimate: Zero-shot High Fidelity Pose Controllable Character Animation</papertitle>
            </a>
            <br><br>
            <a href="" style="color: black;">Bingwen Zhu</a>, &nbsp;
            <a href="" style="color: black;">Fanyi Wang</a>, &nbsp;
            <a href="" style="color: black;"> Peng Liu</a>, &nbsp;     
            <a href="" style="color: black;"> Jingwen Su</a>, &nbsp; 
            <strong style="color: #0F52BA;">Jinxiu Liu</strong>, &nbsp;
            <a href="" style="color: black;">Yanhao Zhang</a>, &nbsp;
            <a href="" style="color: black;">Zuxuan Wu</a>, &nbsp;
            <a href="" style="color: black;"> Yu-Gang Jiang</a>, &nbsp;            
            <a href="" style="color: black;">Guo-Jun Qi</a>, &nbsp;

            <br>
            <br>
            In this paper, we propose PoseAnimate, a novel zero-shot I2V framework for character animation.PoseAnimate contains 
            three key components: 1) Pose-Aware Control Module (PACM) incorporates diverse pose signals into conditional embeddings, 
            to preserve character-independent content and maintain precise alignment of actions.2) Dual Consistency Attention Module 
            (DCAM) enhances temporal consistency, and retains character identity and intricate background details.3) Mask-Guided 
            Decoupling Module (MGDM) refines distinct feature perception, improving animation fidelity by decoupling the character 
            and background.We also propose a Pose Alignment Transition Algorithm (PATA) to ensure smooth action transition.Extensive 
            experiment results demonstrate that our approach 
            outperforms the state-of-the-art training-based methods in terms of character consistency and detail fidelity.           
             <br>
             <br>
             <em style="font-size: 16px;">IJCAI 2024</em>
            <br>
            <a href="https://arxiv.org/pdf/2404.13680.pdf">Paper</a> /
            <a href="https://poseanimate.github.io/">project page</a> 
            <!-- <a href="">paper (Attached by email)</a> /  -->
            <!-- <a href="https://layoutgpt.github.io">project page</a>  -->
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/PiGIE.png" alt="layoutgpt_gif" width="200" height="310 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>üêñPiGIE: Proximal Policy Optimization Guided Diffusion for Fine-Grained Image Editing</papertitle>
            </a>
            <br><br>
            <a href="" style="color: black;">Tiancheng Li*</a>, &nbsp;
            <strong style="color: #0F52BA;">Jinxiu Liu*</strong>, &nbsp;
            <a href="" style="color: black;"> William Luo</a>, &nbsp;     
            <a href="" style="color: black;"> Huajun Chen</a>, &nbsp; 
            <a href="" style="color: black;">Qi Liu</a>, &nbsp;
            <b>(* contribute equally)</b>
            <br>
            <br>
            Instruction-based image editing is a challenging task since it requires to manipulation of the visual content of images according to complex human language instructions. 
            When editing an image with tiny objects and complex positional relationships, existing image editing methods cannot locate the accurate region to execute the editing. 
            To address this issue, we introduce Proximal Policy Optimization Guided Image Editing(PiGIE), a diffusion model that can accurately edit tiny objects in images with complex scenes. 
            The PiGIE is able to incorporate proper noise masks to edit images based on the guidance of the target object‚Äôs attention maps. 
            Different from the traditional image editing approaches based on supervised learning, PiGIE leverages the cosine similarity between UNet‚Äôs attention map 
            and human feedback as a reward function and employs Proximal Policy Optimization (PPO) to fine-tune the diffusion model, such that PiGIE can locate the editing 
            regions precisely based on human instructions. On multiple image editing benchmarks, PiGIE exhibits remarkable improvements in both            
            mage quality and generalization capability. In particular, PiGIE sets a new baseline for editing fine-grained images with multiple tiny objects, 
            shedding light on future studies on text-guided image editing for tiny objects.
            <br>
            <br>
            <em style="font-size: 16px;">ACM MM 2024, under review</em>
            <br>
            <!-- <a href="">paper (Attached by email)</a> /  -->
            <!-- <a href="https://layoutgpt.github.io">project page</a>  -->
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>

        <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SLR.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>Deep Neural Network Compression by Spatial-wise Low-rank
                  Decomposition</papertitle>
              </a>
              <br><br>
              <a href="https://github.com/Xshellye" style="color: black;">Xiaoye Zhu*</a>, &nbsp;
              <strong style="color: #0F52BA;">Jinxiu Liu*</strong>, &nbsp;
              <a href="" style="color: black;">Ye Liu</a>, &nbsp;
              <a href=" https://hkumath.hku.hk/MathWWW/people.php?faculty.mng" style="color: black;">Michael NG</a>, &nbsp;
              <a href="" style="color: black;">Zihan Ji</a>, &nbsp;
              <b>(* contribute equally)</b>
             
              <!-- <a href="https://tsujuifu.github.io/" style="color: black;">Tsu-Jui Fu</a>, &nbsp;
              <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
              <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
              <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
              <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
              <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
              <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
              <br>
              <br>
              In this paper, we introduces a new method for compressing convolutional neural networks (CNNs) based on spatial-wise low-rank decomposition (SLR). The method preserves the higher-order structure of the filter weights and exploits their local low-rankness in different spatial resolutions, which can be implemented as a 1x1 convolution layer and achieves significant reductions in model size and computation cost with minimal accuracy loss. The paper shows the superior performance of the method over state-of-the-art low-rank compression methods and network pruning methods on several popular CNNs and datasets. 
              <br>              
              <br>
              <em style="font-size: 16px;">Applied Intelligence, under review, <font color="#FF0000">positive reviews</font></em>
              <br>
              <!-- <a href="">paper (Attached by email)</a> /  -->
              <!-- <a href="https://layoutgpt.github.io">project page</a>  -->
              <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
              <p></p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>Projects</i></heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="r3cd/img/huggingface.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2305.15393">
              <papertitle>MiniHuggingGPTÔºöA mini multi-modal application like HuggingGPT and MiniGPT-4</papertitle>
            </a>
            <br><br>
            Course Design of Deep Learning and Computer Vision</strong>&nbsp mentored by 
      
            <a href="https://tanmingkui.github.io/" style="color:#0F52BA;">Prof. Mingkui Tan and</a>
            <a href="https://zhuanghp.github.io/" style="color:#0F52BA;">Prof. Huiping Zhuang</a>, &nbsp;
            <!-- <a href="https://varunjampani.github.io/" style="color: black;">Varun Jampani</a>, &nbsp;
            <a href="https://www.arjunakula.com/" style="color: black;">Arjun Akula</a>, &nbsp;
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en" style="color: black;">Xuehai He</a>, &nbsp;
            <a href="https://sites.google.com/site/sugatobasu/" style="color: black;">Sugato Basu</a>, &nbsp;
            <a href="https://eric-xw.github.io/" style="color: black;">Xin Eric Wang</a>, &nbsp;
            <a href="https://sites.cs.ucsb.edu/~william/" style="color: black;">William Yang Wang</a> -->
            <br>
            <br>
            - <b>Took LLM as an API</b> that calls other large-scale models based on <b>natural language instructions</b>.
            <br>
            - Developed a text-based dialogue system based on ChatGLM that can call for three large-scale models for image captioning, image generation, and text conversation using natural language commands by <b>instruction finetuning</b>.
            <br>
            - <b>Provided a webui interface based on gradio</b> for easy interaction with the system and showcased various examples of its capabilities.            <br><br>
            <em style="font-size: 16px;"><b>Awarded as the Best Course Design, 1/39
            </b></em>
            <br>
            <br>
            <!-- <a href="https://arxiv.org/abs/2305.15393">arxiv</a> /  -->
            <a href="https://github.com/Brandon-Liu-Jx/ChatGLM-based-multi-modal-web-UI/blob/main/project_report.pdf">project report</a> 
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>Talk</i></heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/fewshot.png" alt="layoutgpt_gif" width="200" height="150 " style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>Introduction to few-shot learning</papertitle>
            </a>
            <br><br>
           <i>Honored to be invited by <a href="https://xinjie-shen.com/">Xinjie Shen</a>, Chairman of AIA(Artificial Intelligence Association) in SCUT.
           </i><br>
            <br>
            In this talk, I explore the use of few-shot learning techniques for RE based on my research experience. I introduce the basic concepts and principles of few-shot learning, such as the meta-learning framework, the episodic training strategy, and the evaluation metrics. I also discuss some recent advances and applications of few-shot learning for RE, such as the use of pre-trained language models, graph neural networks, contrastive learning, and data augmentation. I demonstrate how few-shot learning can improve the performance and robustness of RE models on different datasets and scenarios. I also share some of the challenges and future directions of few-shot learning for RE.
  
          </em>
          <br>
            <br>
            <!-- <a href="https://arxiv.org/abs/2305.15393">arxiv</a> /  -->
            <a href="assets/slides1.pdf">Slides</a> 
            <!-- / <a href="https://github.com/weixi-feng/LayoutGPT">code</a> -->
            <p></p>
          </td>
        </tr>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="font-size: 26px;"><i>Acknowledgement</i></heading>
          </td>
        </tr>
      </tbody></table>

      I have been fortunate to work as a research intern with these wonderful people who generously provided me with guidance and mentorship.
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>  


          <td style="padding:1%;width:16%;vertical-align:top">
            <p align="center">
            <img style="width:75%" align="center" src="images/scut.png" class="hoverZoomLink">
            <p align="center">
    
            <a href="">@ South China University of Technology</a>
            <p align="left">
              <a href="">Prof. Qi Liu</a>
              <br>
              <a href="">Prof. Ye Liu</a>
              <br>
              <a href="">Prof. Ziqian Zeng</a>
              <br>
              </p>
              </td>
  

          
          <td style="padding:1%;width:16%;vertical-align:top">
            <p align="center">
            <img style="width:90%" align="center" src="images/westlake.png" class="hoverZoomLink">
            <p align="center">
            <a href="">@ Westlake University</a>
            <p align="left">
              <a href="http://maple-lab.net/">Prof. Guo-Jun Qi</a>
              <br>
              <a href="http://maple-lab.net/">Dr. Liyuan Ma</a>
              <br>
              </p>
              </td>
  



          <td style="padding:1%;width:16%;vertical-align:top">
            <p align="">
            <img style="width:100%" align="center" src="images/OPPO.png" class="hoverZoomLink">
            <p align="center">
            <a href="">@ OPPO Research</a>
            <br>
            <p align="left">
              <a href="http://maple-lab.net/">Prof. Guo-Jun Qi</a>
              <br>
            <a href="">Dr. Yanhao Zhang</a>
            <br>
            <a href="">Dr. Fanyi Wang</a>
            <br>
            <a href="">Dr. Jingwen Su</a>
            <br>
            </p>
            </td>

         <td style="padding:1%;width:16%;vertical-align:top">
            <p align="center">
            <img style="width:100%" align="center" src="images/Rochester.png" class="hoverZoomLink">
            <p align="center">
            <a href="">@ University of Rochester</a>
            <br>
            <p align="left">
            <a href="">Prof. Jiebo Luo</a>
            <br>
            <a href="">Yuan Yao</a>
            <br>
            </p>
            </td>          

         <td style="padding:1%;width:16%;vertical-align:top">
            <p align="center">
            <img style="width:80%" align="center" src="images/Peking.png" class="hoverZoomLink">
            <p align="center">
            <a href="">@ Peking University</a>
            <p align="left">
            <a href="">Dr. Weijian Luo</a>
            <br>
         </td>

         <td style="padding:1%;width:16%;vertical-align:top">
          <p align="center">
          <img style="width:80%" align="center" src="images/fudan.png" class="hoverZoomLink">
          <p align="center">
          <a href="">@ Fudan University</a>
          <p align="left">
          <a href="">Bingwen Zhu</a>
          <br>
       </td>

       <td> </td>
       <td> </td>
       <td> </td>
      </tbody></table> 




      
      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading style="font-size: 26px;">Service</heading>
          <p style="font-size: 16px;">
            <li>Reviewer: EACL 2023, ACL 2023, NeurIPS 2023</li>
          </p>
        </td>
      </tr>
      </tbody></table> -->



      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading style="font-size: 26px;">Teaching</heading>
          <p style="font-size: 16px;">
            <li>CS165B Machine Learning, 2020-2021, Spring 2022</li>
            <li>ECE239 Deep Learning, Winter 2019</li>
          </p>
        </td>
      </tr>
      </tbody></table> -->


      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Misc.</heading>
          <p>
            <li>Hong Kong SAR Government Scholarship, 2016-2018</li>
          </p>
        </td>
      </tr>
      </tbody></table> -->

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service & Teaching</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">Reviewer for </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
					
        </tbody></table> -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template from  <a href="https://jonbarron.info/">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
